{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a54c41e-1707-489c-8fc5-3006492e3969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaggu/anaconda3/envs/torch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache cleared at script start!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Not strictly used, but good for general data manipulation\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM, # For sequence-to-sequence models like translation\n",
    "    Seq2SeqTrainingArguments, # Specific training args for seq2seq\n",
    "    Seq2SeqTrainer, # Specific trainer for seq2seq\n",
    "    DataCollatorForSeq2Seq # Handles padding and shifting labels for seq2seq\n",
    ")\n",
    "import evaluate # For metrics like BLEU\n",
    "import numpy as np\n",
    "\n",
    "# --- Crucial for OOM issues: Clear CUDA cache at the very start ---\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared at script start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0c97e6-132e-4378-8abb-f8192a4221e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Configuration Section ---\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Configuration Section ---\")\n",
    "\n",
    "# --- Language Pair ---\n",
    "source_lang = \"en\" # English\n",
    "target_lang = \"fr\" # French\n",
    "# Note: Ensure the model and dataset support these languages.\n",
    "\n",
    "# --- Model & Tokenizer Configuration ---\n",
    "# Using a MarianMT model specifically pre-trained for English to French.\n",
    "# These models are typically named like \"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}\"\n",
    "model_checkpoint = f\"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}\"\n",
    "\n",
    "# Directory to save your fine-tuned model and tokenizer\n",
    "output_model_dir = \"./en-fr-translator-model\"\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "# <<< CRITICAL FOR GPU MEMORY >>>\n",
    "# Start with a small batch size for translation models due to high memory usage.\n",
    "# If OOM, reduce further (e.g., 4, then 2).\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3 # Start with a few epochs. Translation can take time.\n",
    "\n",
    "# Maximum token lengths for input (source) and output (target) sentences.\n",
    "# Long sentences consume more memory. Keep these reasonable.\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "# Determine the device for training (GPU if available, otherwise CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d36f68-c72b-4e7e-99ca-6b2b7d9258a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Data Loading & Initial Inspection ---\n",
      "Loading 'opus_books' dataset for en-fr...\n",
      "Dataset 'en-fr' loaded. Available splits: dict_keys(['train'])\n",
      "Selecting subsets from available splits (max train: 50k, max val/test: 5k)...\n",
      "Info: 'validation' split not found in dataset.\n",
      "Info: 'test' split not found in dataset.\n",
      "No 'validation' or 'test' split found. Creating a small validation set from 'train'.\n",
      "Dataset subset loaded with splits: dict_keys(['train', 'validation'])\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 45000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n",
      "\n",
      "Example from train split:\n",
      "  EN: The Wanderer\n",
      "  FR: Le grand Meaulnes\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Data Loading & Initial Inspection ---\n",
    "print(\"\\n--- 2. Data Loading & Initial Inspection ---\")\n",
    "\n",
    "print(f\"Loading 'opus_books' dataset for {source_lang}-{target_lang}...\")\n",
    "raw_datasets = load_dataset(\"opus_books\", f\"{source_lang}-{target_lang}\")\n",
    "\n",
    "print(f\"Dataset '{source_lang}-{target_lang}' loaded. Available splits: {raw_datasets.keys()}\")\n",
    "\n",
    "# --- IMPORTANT: Select a subset for faster training/testing ---\n",
    "# This section is made robust to handle missing 'validation' or 'test' splits.\n",
    "print(f\"Selecting subsets from available splits (max train: 50k, max val/test: 5k)...\")\n",
    "\n",
    "subset_raw_datasets = DatasetDict()\n",
    "\n",
    "# Process 'train' split\n",
    "if \"train\" in raw_datasets:\n",
    "    subset_raw_datasets[\"train\"] = raw_datasets[\"train\"].select(range(min(len(raw_datasets[\"train\"]), 50000)))\n",
    "else:\n",
    "    print(\"Warning: 'train' split not found in dataset. Cannot proceed without training data.\")\n",
    "    # You might want to exit or raise an error here if 'train' is absolutely necessary.\n",
    "\n",
    "# Process 'validation' split\n",
    "if \"validation\" in raw_datasets:\n",
    "    subset_raw_datasets[\"validation\"] = raw_datasets[\"validation\"].select(range(min(len(raw_datasets[\"validation\"]), 5000)))\n",
    "else:\n",
    "    print(\"Info: 'validation' split not found in dataset.\")\n",
    "\n",
    "# Process 'test' split\n",
    "if \"test\" in raw_datasets:\n",
    "    subset_raw_datasets[\"test\"] = raw_datasets[\"test\"].select(range(min(len(raw_datasets[\"test\"]), 5000)))\n",
    "else:\n",
    "    print(\"Info: 'test' split not found in dataset.\")\n",
    "\n",
    "# Fallback: If 'validation' is missing but 'test' exists, use 'test' as validation\n",
    "if \"train\" in subset_raw_datasets and \"validation\" not in subset_raw_datasets and \"test\" in subset_raw_datasets:\n",
    "    print(\"Using 'test' split as 'validation' split for training as 'validation' was not found.\")\n",
    "    subset_raw_datasets[\"validation\"] = subset_raw_datasets[\"test\"]\n",
    "elif \"train\" in subset_raw_datasets and \"validation\" not in subset_raw_datasets and \"test\" not in subset_raw_datasets:\n",
    "    # If neither validation nor test exists, create a small validation set from train\n",
    "    print(\"No 'validation' or 'test' split found. Creating a small validation set from 'train'.\")\n",
    "    train_size = len(subset_raw_datasets[\"train\"])\n",
    "    val_size = min(int(train_size * 0.1), 5000) # Take 10% of train, max 5000\n",
    "    if train_size > val_size:\n",
    "        subset_raw_datasets[\"validation\"] = subset_raw_datasets[\"train\"].select(range(train_size - val_size, train_size))\n",
    "        subset_raw_datasets[\"train\"] = subset_raw_datasets[\"train\"].select(range(train_size - val_size))\n",
    "    else:\n",
    "        print(\"Warning: Training set too small to create a separate validation split.\")\n",
    "\n",
    "\n",
    "# Assign the subsetted datasets back to raw_datasets for the rest of the script\n",
    "raw_datasets = subset_raw_datasets\n",
    "\n",
    "print(f\"Dataset subset loaded with splits: {raw_datasets.keys()}\")\n",
    "print(raw_datasets) # Show the final structure of the subsetted dataset\n",
    "\n",
    "# Example inspection of the first training sample (make robust to structure)\n",
    "if \"train\" in raw_datasets and len(raw_datasets[\"train\"]) > 0:\n",
    "    print(f\"\\nExample from train split:\")\n",
    "    # Check if the structure is like 'translation' dictionary (opus_books) or direct lang keys (NeelB)\n",
    "    first_example = raw_datasets['train'][0]\n",
    "    if \"translation\" in first_example:\n",
    "        print(f\"  {source_lang.upper()}: {first_example['translation'][source_lang]}\")\n",
    "        print(f\"  {target_lang.upper()}: {first_example['translation'][target_lang]}\")\n",
    "    else: # Assume direct language keys like in NeelB/nepali_parallel_corpus\n",
    "        print(f\"  {source_lang.upper()}: {first_example[source_lang]}\")\n",
    "        print(f\"  {target_lang.upper()}: {first_example[target_lang]}\")\n",
    "else:\n",
    "    print(\"\\nNo 'train' split or empty 'train' split in subsetted dataset for example inspection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6811ad3-41f2-485a-9dc4-127f2eb342ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Tokenizer & Model Initialization ---\n",
      "Loading tokenizer for 'Helsinki-NLP/opus-mt-en-fr'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaggu/anaconda3/envs/torch_env/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 'Helsinki-NLP/opus-mt-en-fr'...\n",
      "Model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- 3. Tokenizer & Model Initialization ---\")\n",
    "\n",
    "# Load AutoTokenizer specific to the chosen model.\n",
    "# This tokenizer knows how to handle both source and target languages for MarianMT.\n",
    "print(f\"Loading tokenizer for '{model_checkpoint}'...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Load AutoModelForSeq2SeqLM: This is the Encoder-Decoder model architecture.\n",
    "print(f\"Loading model '{model_checkpoint}'...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model.to(device) # Move model to GPU/CPU\n",
    "print(\"Model loaded and moved to device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4112cca4-6e48-43ff-b920-3d467f3ba175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Data Preprocessing (Tokenization & Formatting) ---\n",
      "Preprocessing dataset (tokenizing English and French sentences)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaggu/anaconda3/envs/torch_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Tokenizing en-fr dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45000/45000 [00:11<00:00, 3947.97 examples/s]\n",
      "Tokenizing en-fr dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4274.75 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preprocessing complete:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 45000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 4. Data Preprocessing (Tokenization & Formatting) ---\")\n",
    "\n",
    "# Preprocessing function to tokenize both source and target sentences.\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize source language sentences\n",
    "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\" # Pad to max_input_length\n",
    "    )\n",
    "\n",
    "    # Tokenize target language sentences\n",
    "    labels = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    # The tokenizer used for target should often be the same as for the model.\n",
    "    # We also add decoder_input_ids for training.\n",
    "    with tokenizer.as_target_tokenizer(): # Context manager for target language tokenization\n",
    "        labels = tokenizer(\n",
    "            labels,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\" # Pad to max_target_length\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Preprocessing dataset (tokenizing English and French sentences)...\")\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names, # Remove original columns to save memory\n",
    "    desc=f\"Tokenizing {source_lang}-{target_lang} dataset\"\n",
    ")\n",
    "\n",
    "# Set the format to PyTorch tensors for DataLoader\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "print(\"Dataset preprocessing complete:\")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa3ec6b8-4757-4600-8152-74dd9a3b3a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Data Collator & Metrics ---\n",
      "Defining evaluation metrics (BLEU)...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 5. Data Collator & Metrics ---\")\n",
    "\n",
    "# Data collator for sequence-to-sequence tasks.\n",
    "# It performs dynamic padding and shifts labels for the decoder.\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Define Evaluation Metrics (BLEU score)\n",
    "print(\"Defining evaluation metrics (BLEU)...\")\n",
    "metric = evaluate.load(\"sacrebleu\") # sacrebleu is a robust BLEU implementation\n",
    "\n",
    "# Function to compute metrics during evaluation.\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # If the model outputs tuples, take the first element (logits)\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 (Hugging Face's default for padding in labels) with tokenizer's pad_token_id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Post-process the decoded texts (e.g., remove extra whitespace)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels] # sacrebleu expects list of lists for references\n",
    "\n",
    "    # Compute BLEU score\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]} # 'score' key contains the BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7414187-0196-4250-9f32-af8f003a96bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Training Setup & Execution ---\n",
      "\n",
      "--- Training Process Initiated ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22605/1482298765.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 21:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.322316</td>\n",
       "      <td>37.106666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaggu/anaconda3/envs/torch_env/lib/python3.10/site-packages/transformers/modeling_utils.py:3685: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete!\n",
      "Fine-tuned model and tokenizer saved to: ./en-fr-translator-model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 6. Training Setup & Execution ---\")\n",
    "\n",
    "# Define Training Arguments for the Hugging Face Trainer.\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_model_dir,\n",
    "    eval_strategy=\"epoch\",  # Evaluate at the end of each training epoch\n",
    "    save_strategy=\"epoch\",        # Save model checkpoint at the end of each epoch\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=1, #NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,   # IMPORTANT: Enables generation during evaluation steps\n",
    "    fp16=torch.cuda.is_available(), # Enable mixed precision training if GPU available (faster, less VRAM)\n",
    "    report_to=\"none\",             # Disables integration with external logging tools\n",
    "    load_best_model_at_end=True,  # Load the best model based on metric_for_best_model\n",
    "    metric_for_best_model=\"bleu\", # Monitor BLEU score for selecting the best model\n",
    "    greater_is_better=True,       # For BLEU, a higher score is better\n",
    "    gradient_checkpointing=True,  # <<< IMPORTANT FOR GPU MEMORY >>> Reduces memory by recomputing activations.\n",
    ")\n",
    "\n",
    "# Initialize the Hugging Face Trainer.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start the Training Process.\n",
    "print(\"\\n--- Training Process Initiated ---\")\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    print(\"\\nTraining complete!\")\n",
    "    # Save the fine-tuned model and its tokenizer\n",
    "    trainer.save_model(output_model_dir)\n",
    "    tokenizer.save_pretrained(output_model_dir) # Save the tokenizer too!\n",
    "    print(f\"Fine-tuned model and tokenizer saved to: {output_model_dir}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nERROR: Training failed due to a RuntimeError: {e}\")\n",
    "    print(\"This often indicates a CUDA out-of-memory error. Try reducing BATCH_SIZE or max_input/target_length.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: An unexpected error occurred during training: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7637c97-9e4e-4b90-a1b0-2b9dde784f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. Final Evaluation & Inference ---\n",
      "No 'test' split available for final evaluation.\n",
      "\n",
      "--- Quick Inference Test with Pipeline ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaggu/anaconda3/envs/torch_env/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating sample texts:\n",
      "  Original en: Hello, how are you today?\n",
      "  Translated fr: Bonjour, comment allez-vous aujourd'hui?\n",
      "------------------------------\n",
      "  Original en: This is a great example of machine translation.\n",
      "  Translated fr: Cest un grand exemple de traduction automatique.\n",
      "------------------------------\n",
      "  Original en: The quick brown fox jumps over the lazy dog.\n",
      "  Translated fr: Le rapide renard brun saute sur le chien paresseux.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 7. Final Evaluation & Inference ---\")\n",
    "\n",
    "# Evaluate the model on the held-out test set (if available).\n",
    "if \"test\" in tokenized_datasets:\n",
    "    print(\"Evaluating on the held-out test set...\")\n",
    "    test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "    print(f\"Test Set Evaluation Results: {test_results}\")\n",
    "else:\n",
    "    print(\"No 'test' split available for final evaluation.\")\n",
    "\n",
    "print(\"\\n--- Quick Inference Test with Pipeline ---\")\n",
    "from transformers import pipeline\n",
    "\n",
    "try:\n",
    "    # Load the fine-tuned model and tokenizer into a Hugging Face pipeline.\n",
    "    # The 'translation_en_to_fr' pipeline automatically handles tokenization,\n",
    "    # model inference, and text decoding for translation.\n",
    "    translator = pipeline(\n",
    "        f\"translation_{source_lang}_to_{target_lang}\",\n",
    "        model=output_model_dir, # Loads your saved model\n",
    "        tokenizer=output_model_dir, # Loads your saved tokenizer\n",
    "        device=0 if torch.cuda.is_available() else -1 # Use GPU 0 if available, else CPU\n",
    "    )\n",
    "\n",
    "    sample_texts = [\n",
    "        \"Hello, how are you today?\",\n",
    "        \"This is a great example of machine translation.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\nTranslating sample texts:\")\n",
    "    for i, text in enumerate(sample_texts):\n",
    "        translation = translator(text)\n",
    "        print(f\"  Original {source_lang}: {text}\")\n",
    "        print(f\"  Translated {target_lang}: {translation[0]['translation_text']}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR during quick inference test: {e}\")\n",
    "    print(\"Please ensure the model and tokenizer were saved correctly and `pipeline` can be initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e4403-8116-4a1d-98d1-120d6442cace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34307401-2bcb-4c80-b583-aae4cb1fc208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c7f4d-b6aa-42e0-8de1-3b1e919e4ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
